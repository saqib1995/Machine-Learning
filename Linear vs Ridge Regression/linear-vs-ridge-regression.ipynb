{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear versus Ridge Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Program that illustrates the difference in training and test costs for both linear and ridge regression on the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a dataset that is part of the sklearn.dataset package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  Getting, understanding, and preprocessing the dataset\n",
    "\n",
    "We first import the standard libaries and some libraries that will help us scale the data and perform some \"feature engineering\" by transforming the data into $\\Phi_2({\\bf x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the boston dataset from sklearn\n",
    "boston_data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:  (506, 13)\n",
      "y shape:  (506,)\n",
      "y after reshape:  (506, 1)\n",
      "The number of features is:  13\n",
      "The features:  ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "The number of exampels in our dataset:  506\n",
      "[[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
      "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
      "  4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
      "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
      "  9.1400e+00]]\n"
     ]
    }
   ],
   "source": [
    "#  Create X and Y variables - X holding the .data and Y holding .target \n",
    "X = boston_data.data\n",
    "y = boston_data.target\n",
    "print(\"x shape: \", X.shape)\n",
    "print(\"y shape: \", y.shape)\n",
    "#  Reshape Y to be a rank 2 matrix \n",
    "y = y.reshape(X.shape[0], 1)\n",
    "print(\"y after reshape: \", y.shape)\n",
    "# Observe the number of features and the number of labels\n",
    "print('The number of features is: ', X.shape[1])\n",
    "# Printing out the features\n",
    "print('The features: ', boston_data.feature_names)\n",
    "# The number of examples\n",
    "print('The number of exampels in our dataset: ', X.shape[0])\n",
    "#Observing the first 2 rows of the data\n",
    "print(X[0:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create polynomial features for the dataset to test linear and ridge regression on data with d = 1 and data with d = 2. We can also increase the # of degress and see what effect it has on the training and test error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PolynomialFeatures object with degree = 2. \n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_2 = poly.fit_transform(X)\n",
    "y_2 = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 105)\n",
      "(506, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_2.shape)\n",
    "print(y_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(X, w):\n",
    "    yp = X @ w;\n",
    "    return yp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_coeff_ridge_normaleq(X_train, y_train, alpha):\n",
    "    xt = X_train.T\n",
    "    rows = X_train.shape[1]\n",
    "    I = np.identity(rows)\n",
    "    alphaI = alpha * I;\n",
    "    xtx = xt @ X_train\n",
    "    inv = np.linalg.pinv(xtx + alphaI)\n",
    "    w = inv @ xt @ y_train\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_err(X_train, X_test, y_train, y_test, w): \n",
    "    \n",
    "    train_prediction = get_prediction(X_train, w)\n",
    "    test_prediction = get_prediction(X_test, w)\n",
    "    train_error = np.mean(np.square(train_prediction - y_train),axis = 0)\n",
    "    test_error = np.mean(np.square(test_prediction - y_test),axis = 0)\n",
    "    \n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def k_fold_cross_validation(k, X, y, alpha):\n",
    "    kf = KFold(n_splits=k, random_state=21, shuffle=True)\n",
    "    total_E_val_test = 0\n",
    "    total_E_val_train = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # centering the data so we do not need the intercept term (we could have also chose w_0=average y value)\n",
    "        y_train_mean = np.mean(y_train)\n",
    "        y_train = y_train - y_train_mean\n",
    "        y_test = y_test - y_train_mean\n",
    "        # scaling the data matrix\n",
    "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "          \n",
    "        w_ridge = get_coeff_ridge_normaleq(X_train, y_train, alpha)\n",
    "        ridge_train_err, ridge_test_err = evaluation_err(X_train, X_test, y_train, y_test, w_ridge)\n",
    "        total_E_val_test += ridge_test_err\n",
    "        total_E_val_train += ridge_train_err\n",
    "       \n",
    "    E_val_test = total_E_val_test/k\n",
    "    E_val_train = total_E_val_train/k\n",
    "    return  E_val_test, E_val_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 13)\n"
     ]
    }
   ],
   "source": [
    "myx = np.array([[5, 0.5, 2, 0, 4, 8, 4, 6, 2, 2, 2, 4, 5.5]])\n",
    "print(myx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda:  0.0\n",
      "avg test err:  [23.63606861]\n",
      "avg train err:  [21.80618358]\n",
      "*************************************************************************************\n",
      "lambda:  10.0\n",
      "avg test err:  [23.688583]\n",
      "avg train err:  [21.89290116]\n",
      "*************************************************************************************\n",
      "lambda:  31.622776601683793\n",
      "avg test err:  [24.0178403]\n",
      "avg train err:  [22.28544406]\n",
      "*************************************************************************************\n",
      "lambda:  100.0\n",
      "avg test err:  [25.29385255]\n",
      "avg train err:  [23.72548838]\n",
      "*************************************************************************************\n",
      "lambda:  316.22776601683796\n",
      "avg test err:  [29.45729622]\n",
      "avg train err:  [28.1665541]\n",
      "*************************************************************************************\n",
      "lambda:  1000.0\n",
      "avg test err:  [39.48949336]\n",
      "avg train err:  [38.53214873]\n",
      "*************************************************************************************\n",
      "lambda:  3162.2776601683795\n",
      "avg test err:  [54.64719108]\n",
      "avg train err:  [54.0070265]\n",
      "*************************************************************************************\n",
      "lambda:  10000.0\n",
      "avg test err:  [69.71851751]\n",
      "avg train err:  [69.25978171]\n",
      "*************************************************************************************\n",
      "lambda:  31622.776601683792\n",
      "avg test err:  [78.9216211]\n",
      "avg train err:  [78.53074244]\n",
      "*************************************************************************************\n",
      "lambda:  100000.0\n",
      "avg test err:  [82.77240429]\n",
      "avg train err:  [82.40325012]\n",
      "*************************************************************************************\n",
      "lambda:  316227.7660168379\n",
      "avg test err:  [84.11748626]\n",
      "avg train err:  [83.75514888]\n",
      "*************************************************************************************\n",
      "lambda:  1000000.0\n",
      "avg test err:  [84.55699418]\n",
      "avg train err:  [84.19680335]\n",
      "*************************************************************************************\n",
      "lambda:  3162277.6601683795\n",
      "avg test err:  [84.69744416]\n",
      "avg train err:  [84.33793108]\n",
      "*************************************************************************************\n",
      "lambda:  10000000.0\n",
      "avg test err:  [84.74200651]\n",
      "avg train err:  [84.38270764]\n",
      "*************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "lambdas = np.logspace(1,7, num = 13)\n",
    "lambdas = np.insert(lambdas, 0, 0, axis = 0)\n",
    "for lam in lambdas:\n",
    "    print(\"lambda: \", lam)\n",
    "    E_val_test, E_val_train = k_fold_cross_validation(10, X, y, lam)\n",
    "    print(\"avg test err: \", E_val_test)\n",
    "    print(\"avg train err: \", E_val_train)\n",
    "    print(\"*************************************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda:  0.0\n",
      "avg test err:  [11.85496823]\n",
      "avg train err:  [5.80882082]\n",
      "*************************************************************************************\n",
      "lambda:  10.0\n",
      "avg test err:  [13.476138]\n",
      "avg train err:  [10.04905587]\n",
      "*************************************************************************************\n",
      "lambda:  31.622776601683793\n",
      "avg test err:  [15.82960197]\n",
      "avg train err:  [12.75170627]\n",
      "*************************************************************************************\n",
      "lambda:  100.0\n",
      "avg test err:  [18.98001882]\n",
      "avg train err:  [16.22269059]\n",
      "*************************************************************************************\n",
      "lambda:  316.22776601683796\n",
      "avg test err:  [22.06869231]\n",
      "avg train err:  [19.70025365]\n",
      "*************************************************************************************\n",
      "lambda:  1000.0\n",
      "avg test err:  [26.21847559]\n",
      "avg train err:  [24.28745798]\n",
      "*************************************************************************************\n",
      "lambda:  3162.2776601683795\n",
      "avg test err:  [34.58026602]\n",
      "avg train err:  [33.08666832]\n",
      "*************************************************************************************\n",
      "lambda:  10000.0\n",
      "avg test err:  [47.78952059]\n",
      "avg train err:  [46.76199936]\n",
      "*************************************************************************************\n",
      "lambda:  31622.776601683792\n",
      "avg test err:  [62.64628881]\n",
      "avg train err:  [62.00900192]\n",
      "*************************************************************************************\n",
      "lambda:  100000.0\n",
      "avg test err:  [74.73795161]\n",
      "avg train err:  [74.28758895]\n",
      "*************************************************************************************\n",
      "lambda:  316227.7660168379\n",
      "avg test err:  [81.07998619]\n",
      "avg train err:  [80.69255135]\n",
      "*************************************************************************************\n",
      "lambda:  1000000.0\n",
      "avg test err:  [83.53524235]\n",
      "avg train err:  [83.1672357]\n",
      "*************************************************************************************\n",
      "lambda:  3162277.6601683795\n",
      "avg test err:  [84.36776579]\n",
      "avg train err:  [84.0057958]\n",
      "*************************************************************************************\n",
      "lambda:  10000000.0\n",
      "avg test err:  [84.63708052]\n",
      "avg train err:  [84.27700626]\n",
      "*************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for lam in lambdas:\n",
    "    print(\"lambda: \", lam)\n",
    "    E_val_test, E_val_train = k_fold_cross_validation(10, X_2, y_2, lam)\n",
    "    print(\"avg test err: \", E_val_test)\n",
    "    print(\"avg train err: \", E_val_train)\n",
    "    print(\"*************************************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we get least error with degree 2 when lambda = 0 i.e. for linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24. ]\n",
      " [21.6]\n",
      " [34.7]\n",
      " [33.4]\n",
      " [36.2]\n",
      " [28.7]\n",
      " [22.9]\n",
      " [27.1]\n",
      " [16.5]\n",
      " [18.9]\n",
      " [15. ]\n",
      " [18.9]\n",
      " [21.7]\n",
      " [20.4]\n",
      " [18.2]\n",
      " [19.9]\n",
      " [23.1]\n",
      " [17.5]\n",
      " [20.2]\n",
      " [18.2]\n",
      " [13.6]\n",
      " [19.6]\n",
      " [15.2]\n",
      " [14.5]\n",
      " [15.6]\n",
      " [13.9]\n",
      " [16.6]\n",
      " [14.8]\n",
      " [18.4]\n",
      " [21. ]\n",
      " [12.7]\n",
      " [14.5]\n",
      " [13.2]\n",
      " [13.1]\n",
      " [13.5]\n",
      " [18.9]\n",
      " [20. ]\n",
      " [21. ]\n",
      " [24.7]\n",
      " [30.8]\n",
      " [34.9]\n",
      " [26.6]\n",
      " [25.3]\n",
      " [24.7]\n",
      " [21.2]\n",
      " [19.3]\n",
      " [20. ]\n",
      " [16.6]\n",
      " [14.4]\n",
      " [19.4]\n",
      " [19.7]\n",
      " [20.5]\n",
      " [25. ]\n",
      " [23.4]\n",
      " [18.9]\n",
      " [35.4]\n",
      " [24.7]\n",
      " [31.6]\n",
      " [23.3]\n",
      " [19.6]\n",
      " [18.7]\n",
      " [16. ]\n",
      " [22.2]\n",
      " [25. ]\n",
      " [33. ]\n",
      " [23.5]\n",
      " [19.4]\n",
      " [22. ]\n",
      " [17.4]\n",
      " [20.9]\n",
      " [24.2]\n",
      " [21.7]\n",
      " [22.8]\n",
      " [23.4]\n",
      " [24.1]\n",
      " [21.4]\n",
      " [20. ]\n",
      " [20.8]\n",
      " [21.2]\n",
      " [20.3]\n",
      " [28. ]\n",
      " [23.9]\n",
      " [24.8]\n",
      " [22.9]\n",
      " [23.9]\n",
      " [26.6]\n",
      " [22.5]\n",
      " [22.2]\n",
      " [23.6]\n",
      " [28.7]\n",
      " [22.6]\n",
      " [22. ]\n",
      " [22.9]\n",
      " [25. ]\n",
      " [20.6]\n",
      " [28.4]\n",
      " [21.4]\n",
      " [38.7]\n",
      " [43.8]\n",
      " [33.2]\n",
      " [27.5]\n",
      " [26.5]\n",
      " [18.6]\n",
      " [19.3]\n",
      " [20.1]\n",
      " [19.5]\n",
      " [19.5]\n",
      " [20.4]\n",
      " [19.8]\n",
      " [19.4]\n",
      " [21.7]\n",
      " [22.8]\n",
      " [18.8]\n",
      " [18.7]\n",
      " [18.5]\n",
      " [18.3]\n",
      " [21.2]\n",
      " [19.2]\n",
      " [20.4]\n",
      " [19.3]\n",
      " [22. ]\n",
      " [20.3]\n",
      " [20.5]\n",
      " [17.3]\n",
      " [18.8]\n",
      " [21.4]\n",
      " [15.7]\n",
      " [16.2]\n",
      " [18. ]\n",
      " [14.3]\n",
      " [19.2]\n",
      " [19.6]\n",
      " [23. ]\n",
      " [18.4]\n",
      " [15.6]\n",
      " [18.1]\n",
      " [17.4]\n",
      " [17.1]\n",
      " [13.3]\n",
      " [17.8]\n",
      " [14. ]\n",
      " [14.4]\n",
      " [13.4]\n",
      " [15.6]\n",
      " [11.8]\n",
      " [13.8]\n",
      " [15.6]\n",
      " [14.6]\n",
      " [17.8]\n",
      " [15.4]\n",
      " [21.5]\n",
      " [19.6]\n",
      " [15.3]\n",
      " [19.4]\n",
      " [17. ]\n",
      " [15.6]\n",
      " [13.1]\n",
      " [41.3]\n",
      " [24.3]\n",
      " [23.3]\n",
      " [27. ]\n",
      " [50. ]\n",
      " [50. ]\n",
      " [50. ]\n",
      " [22.7]\n",
      " [25. ]\n",
      " [50. ]\n",
      " [23.8]\n",
      " [23.8]\n",
      " [22.3]\n",
      " [17.4]\n",
      " [19.1]\n",
      " [23.1]\n",
      " [23.6]\n",
      " [22.6]\n",
      " [29.4]\n",
      " [23.2]\n",
      " [24.6]\n",
      " [29.9]\n",
      " [37.2]\n",
      " [39.8]\n",
      " [36.2]\n",
      " [37.9]\n",
      " [32.5]\n",
      " [26.4]\n",
      " [29.6]\n",
      " [50. ]\n",
      " [32. ]\n",
      " [29.8]\n",
      " [34.9]\n",
      " [37. ]\n",
      " [30.5]\n",
      " [36.4]\n",
      " [31.1]\n",
      " [29.1]\n",
      " [50. ]\n",
      " [33.3]\n",
      " [30.3]\n",
      " [34.6]\n",
      " [34.9]\n",
      " [32.9]\n",
      " [24.1]\n",
      " [42.3]\n",
      " [48.5]\n",
      " [50. ]\n",
      " [22.6]\n",
      " [24.4]\n",
      " [22.5]\n",
      " [24.4]\n",
      " [20. ]\n",
      " [21.7]\n",
      " [19.3]\n",
      " [22.4]\n",
      " [28.1]\n",
      " [23.7]\n",
      " [25. ]\n",
      " [23.3]\n",
      " [28.7]\n",
      " [21.5]\n",
      " [23. ]\n",
      " [26.7]\n",
      " [21.7]\n",
      " [27.5]\n",
      " [30.1]\n",
      " [44.8]\n",
      " [50. ]\n",
      " [37.6]\n",
      " [31.6]\n",
      " [46.7]\n",
      " [31.5]\n",
      " [24.3]\n",
      " [31.7]\n",
      " [41.7]\n",
      " [48.3]\n",
      " [29. ]\n",
      " [24. ]\n",
      " [25.1]\n",
      " [31.5]\n",
      " [23.7]\n",
      " [23.3]\n",
      " [22. ]\n",
      " [20.1]\n",
      " [22.2]\n",
      " [23.7]\n",
      " [17.6]\n",
      " [18.5]\n",
      " [24.3]\n",
      " [20.5]\n",
      " [24.5]\n",
      " [26.2]\n",
      " [24.4]\n",
      " [24.8]\n",
      " [29.6]\n",
      " [42.8]\n",
      " [21.9]\n",
      " [20.9]\n",
      " [44. ]\n",
      " [50. ]\n",
      " [36. ]\n",
      " [30.1]\n",
      " [33.8]\n",
      " [43.1]\n",
      " [48.8]\n",
      " [31. ]\n",
      " [36.5]\n",
      " [22.8]\n",
      " [30.7]\n",
      " [50. ]\n",
      " [43.5]\n",
      " [20.7]\n",
      " [21.1]\n",
      " [25.2]\n",
      " [24.4]\n",
      " [35.2]\n",
      " [32.4]\n",
      " [32. ]\n",
      " [33.2]\n",
      " [33.1]\n",
      " [29.1]\n",
      " [35.1]\n",
      " [45.4]\n",
      " [35.4]\n",
      " [46. ]\n",
      " [50. ]\n",
      " [32.2]\n",
      " [22. ]\n",
      " [20.1]\n",
      " [23.2]\n",
      " [22.3]\n",
      " [24.8]\n",
      " [28.5]\n",
      " [37.3]\n",
      " [27.9]\n",
      " [23.9]\n",
      " [21.7]\n",
      " [28.6]\n",
      " [27.1]\n",
      " [20.3]\n",
      " [22.5]\n",
      " [29. ]\n",
      " [24.8]\n",
      " [22. ]\n",
      " [26.4]\n",
      " [33.1]\n",
      " [36.1]\n",
      " [28.4]\n",
      " [33.4]\n",
      " [28.2]\n",
      " [22.8]\n",
      " [20.3]\n",
      " [16.1]\n",
      " [22.1]\n",
      " [19.4]\n",
      " [21.6]\n",
      " [23.8]\n",
      " [16.2]\n",
      " [17.8]\n",
      " [19.8]\n",
      " [23.1]\n",
      " [21. ]\n",
      " [23.8]\n",
      " [23.1]\n",
      " [20.4]\n",
      " [18.5]\n",
      " [25. ]\n",
      " [24.6]\n",
      " [23. ]\n",
      " [22.2]\n",
      " [19.3]\n",
      " [22.6]\n",
      " [19.8]\n",
      " [17.1]\n",
      " [19.4]\n",
      " [22.2]\n",
      " [20.7]\n",
      " [21.1]\n",
      " [19.5]\n",
      " [18.5]\n",
      " [20.6]\n",
      " [19. ]\n",
      " [18.7]\n",
      " [32.7]\n",
      " [16.5]\n",
      " [23.9]\n",
      " [31.2]\n",
      " [17.5]\n",
      " [17.2]\n",
      " [23.1]\n",
      " [24.5]\n",
      " [26.6]\n",
      " [22.9]\n",
      " [24.1]\n",
      " [18.6]\n",
      " [30.1]\n",
      " [18.2]\n",
      " [20.6]\n",
      " [17.8]\n",
      " [21.7]\n",
      " [22.7]\n",
      " [22.6]\n",
      " [25. ]\n",
      " [19.9]\n",
      " [20.8]\n",
      " [16.8]\n",
      " [21.9]\n",
      " [27.5]\n",
      " [21.9]\n",
      " [23.1]\n",
      " [50. ]\n",
      " [50. ]\n",
      " [50. ]\n",
      " [50. ]\n",
      " [50. ]\n",
      " [13.8]\n",
      " [13.8]\n",
      " [15. ]\n",
      " [13.9]\n",
      " [13.3]\n",
      " [13.1]\n",
      " [10.2]\n",
      " [10.4]\n",
      " [10.9]\n",
      " [11.3]\n",
      " [12.3]\n",
      " [ 8.8]\n",
      " [ 7.2]\n",
      " [10.5]\n",
      " [ 7.4]\n",
      " [10.2]\n",
      " [11.5]\n",
      " [15.1]\n",
      " [23.2]\n",
      " [ 9.7]\n",
      " [13.8]\n",
      " [12.7]\n",
      " [13.1]\n",
      " [12.5]\n",
      " [ 8.5]\n",
      " [ 5. ]\n",
      " [ 6.3]\n",
      " [ 5.6]\n",
      " [ 7.2]\n",
      " [12.1]\n",
      " [ 8.3]\n",
      " [ 8.5]\n",
      " [ 5. ]\n",
      " [11.9]\n",
      " [27.9]\n",
      " [17.2]\n",
      " [27.5]\n",
      " [15. ]\n",
      " [17.2]\n",
      " [17.9]\n",
      " [16.3]\n",
      " [ 7. ]\n",
      " [ 7.2]\n",
      " [ 7.5]\n",
      " [10.4]\n",
      " [ 8.8]\n",
      " [ 8.4]\n",
      " [16.7]\n",
      " [14.2]\n",
      " [20.8]\n",
      " [13.4]\n",
      " [11.7]\n",
      " [ 8.3]\n",
      " [10.2]\n",
      " [10.9]\n",
      " [11. ]\n",
      " [ 9.5]\n",
      " [14.5]\n",
      " [14.1]\n",
      " [16.1]\n",
      " [14.3]\n",
      " [11.7]\n",
      " [13.4]\n",
      " [ 9.6]\n",
      " [ 8.7]\n",
      " [ 8.4]\n",
      " [12.8]\n",
      " [10.5]\n",
      " [17.1]\n",
      " [18.4]\n",
      " [15.4]\n",
      " [10.8]\n",
      " [11.8]\n",
      " [14.9]\n",
      " [12.6]\n",
      " [14.1]\n",
      " [13. ]\n",
      " [13.4]\n",
      " [15.2]\n",
      " [16.1]\n",
      " [17.8]\n",
      " [14.9]\n",
      " [14.1]\n",
      " [12.7]\n",
      " [13.5]\n",
      " [14.9]\n",
      " [20. ]\n",
      " [16.4]\n",
      " [17.7]\n",
      " [19.5]\n",
      " [20.2]\n",
      " [21.4]\n",
      " [19.9]\n",
      " [19. ]\n",
      " [19.1]\n",
      " [19.1]\n",
      " [20.1]\n",
      " [19.9]\n",
      " [19.6]\n",
      " [23.2]\n",
      " [29.8]\n",
      " [13.8]\n",
      " [13.3]\n",
      " [16.7]\n",
      " [12. ]\n",
      " [14.6]\n",
      " [21.4]\n",
      " [23. ]\n",
      " [23.7]\n",
      " [25. ]\n",
      " [21.8]\n",
      " [20.6]\n",
      " [21.2]\n",
      " [19.1]\n",
      " [20.6]\n",
      " [15.2]\n",
      " [ 7. ]\n",
      " [ 8.1]\n",
      " [13.6]\n",
      " [20.1]\n",
      " [21.8]\n",
      " [24.5]\n",
      " [23.1]\n",
      " [19.7]\n",
      " [18.3]\n",
      " [21.2]\n",
      " [17.5]\n",
      " [16.8]\n",
      " [22.4]\n",
      " [20.6]\n",
      " [23.9]\n",
      " [22. ]\n",
      " [11.9]]\n",
      "using lambda = 0 and degree 2\n",
      "for x = [5, 0.5, 2, 0, 4, 8, 4, 6, 2, 2, 2, 4, 5.5]\n",
      "preidicted price:  [[253.15609481]]\n"
     ]
    }
   ],
   "source": [
    "myx_2 = poly.transform(myx)\n",
    "\n",
    "print(y_2)\n",
    "y_mean = np.mean(y)\n",
    "y_scaled = y - y_mean\n",
    "\n",
    "# scaling the data matrix\n",
    "scaler = preprocessing.StandardScaler().fit(X_2)\n",
    "X_scaled = scaler.transform(X_2)\n",
    "myx_scaled = scaler.transform(myx_2)\n",
    "        \n",
    "curr_w = get_coeff_ridge_normaleq(X_scaled, y_scaled, 0)\n",
    "myx_prediction = get_prediction(myx_scaled, curr_w)\n",
    "# predicted_price = scaler.inverse_transform(myx_prediction)\n",
    "print(\"using lambda = 0 and degree 2\")\n",
    "print(\"for x = [5, 0.5, 2, 0, 4, 8, 4, 6, 2, 2, 2, 4, 5.5]\")\n",
    "print(\"preidicted price: \", myx_prediction + y_mean) # +y_train_mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
